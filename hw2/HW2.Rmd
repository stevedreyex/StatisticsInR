# HW2 by 311554032

## 1.

Pizza was original invented in Naples, Italy in the early 19th century. It is a kind of flat bread baked by oven and is usually topped with cheese, tomato sauce, meat and vegetables. Pizza has become a common delicacy around the world.\
Suppose the dataset **pizza2.txt** contains data on pizzas sold at a US pizzeria, which could provide insights into factors that influence pizza ratings. The table below shows some key information about the data.

![](fig.hw2.1.png)\
**Tasks:** I. "Using coal to bake pizzas yields different ratings with those baked by using gas or wood". We wish to verify this statement by providing some statistical evidences:

### I.a

Compute each of the average ratings of the pizzas baked by coal, wood and gas, along with the standard deviations of the ratings. Comment the results. [hint: you could use codes like **pizza[pizza[,"heat"]=="Coal", ratings] OR sapply()** and a self-defined function to do so]

```{r}
pizza <- read.table("pizza2.txt",header=TRUE)
df <- data.frame(pizza)
```

It's correct that there's only 3 types of heat source

```{r echo=TRUE}
gas_data <- df[df$heat == "Gas", ]
coal_data <- df[df$heat == "Coal", ]
wood_data <- df[df$heat == "Wood", ]
mean(gas_data$rating)
mean(coal_data$rating)
mean(wood_data$rating)
sd(gas_data$rating)
sd(coal_data$rating)
sd(wood_data$rating)
```

### I.b

Perform an ANOVA test to find out if the ratings of the pizzas baked by different heat sources are equal in average. Comment the results.

```{r}
rating = df$rating
heat = df$heat

fm1=aov(rating~heat,data=df)
summary(fm1)

```

```{r}
boxplot(rating~heat,data=df)
```

Comments:

### I.c

Fit a simple linear regression by using **rating** as the response variable and **heat** as the predictor variable. Interpret the estimated regression coefficients and the corresponding p- values.

```{r}
lm.fit <-lm(rating ~ heat , data=df)
summary(lm.fit)
coef(lm.fit)
```

### I.d

Compare and contrast the results in (a), (b) and (c). In other words, what information are shown from both analyses, OR from one analysis, but not from the others?

**A:**

In Common: All need contiguous variable (such as rating)

Different: 1. ANOVA and simple linear regression test for differences between groups. 2. ANOVA is used for categorical variables, while simple linear regression is used for continuous independent variables.

II\. Fit two multiple linear regression by using **rating** as the response variable, and

### II.e

**heat**, **area** and **cost** as the predictor variables.

```{r}
lm.fit <-lm(df$rating ~ df$heat + df$area + df$cost , data=df)
summary(lm.fit)
coef(lm.fit)
```

### II.f

**heat_re**, **area** and **cost** as the predictor variables.

```{r}
lm.fit <-lm(df$rating ~ df$heat_re + df$area + df$cost , data=df)
summary(lm.fit)
coef(lm.fit)
```

<div>

Assume that coal-baked pizzas produce the highest ratings, followed by using wood, and then gas, compare the two models. It is more reasonable to use dummy (indicator) variables in model fitting (as in 1b.), why? Justify your answer by comparing the interpretations of the regression coefficients of **heat** and **heat_re**.

</div>

**A: It is better to use Heat and** make all types of heat as a individual dummy variable. By using heat_re, by the observation of II.e and II.f we can't clearly see the information from the coefficient, **that is because the distance of each type in heat_re is not equal**, so it is not easy to tell which heat is better from the coefficient.

<div>

Then, predict the rating for a coal baked pizza that costs \$2.50 per slice in LittleItaly and find the corresponding prediction interval using both of the models built in 3a. and 3b. [hint: use **predict()**]

</div>

("heat" 為 categorical variable, "heat_re" 則是 numerical. 在這題用兩個方法跑迴歸, 再比較兩者結果.)

```{r}
y = df$rating
x1 = df$heat
x2 = df$area
x3 = df$cost
x4 = df$heat_re
md_predict1 <-lm(y ~ x1 + x2 + x3 , data=df)
md_predict2 <-lm(y ~ x4 + x2 + x3 , data=df)
p_area = c("LittleItaly")
p_cost = c(2.50)
p_heat = c("Coal")
p_brick = c(TRUE) # Don't care
p_heat_re = c(0)
new_data <- data.frame(x2 = p_area, x3 = p_cost, x1 = p_heat, x5 = p_brick, x4 = p_heat_re)
new_predictions <- predict(md_predict1, newdata = new_data)
new_predictions
new_predictions <- predict(md_predict2, newdata = new_data)
new_predictions
```

Comparison: result of heat_re is little bit higher than attribute heat

III\. Construct the 95% t-based confidence intervals for the mean rating for each pizzeria location (**area**). Plot **all** of the intervals in a single plot and briefly comment the results. (Hint: you could make use of **plot(), lines()** and **points() OR** search online1 for some ways to plot confidence intervals.)

First: Mean rating for each pizzeria location (**area**)

```{r}
means <- aggregate(y ~ x2, data = df, mean)
means
```

Second: Calculate the standard error and confidence intervals for each mean rating

```{r}
n <- length(y)
se <- tapply(y, x2, sd)/sqrt(n)
ci <- tapply(y, x2, function(x) t.test(x)$conf.int)
ci
```

Third: Plot

```{r}
# create a scatterplot of the mean ratings
plot(means$y, xlim = c(0, max(means$y) + 1), ylim = c(0, length(means$x2) + 1),
     xlab = "Mean Rating", ylab = "Location", pch = 19)

# add points for each location
points(means$y, 1:length(means$x2), pch = 19)

# extract the lower and upper bounds of each confidence interval
lower <- sapply(ci, function(x) x[1])
upper <- sapply(ci, function(x) x[2])

# add line segments for each confidence interval
segments(lower, 1:length(means$x2), upper, 1:length(means$x2), lwd = 2)

```

練習畫信賴區間 (後來的 project 可以使用類似方法呈現資料的比較)

## 2.

Suppose we are interested in studying the effect of different types of music on people's moods. We collect data on 60 participants and record their mood score (out of 10) after listening to one of three types of music: classical, jazz, or pop. In the file "mood.csv" The data look like

![](fig.hw2.2.png)

Let Y be "MoodScore" and "MusicType" and "Gender" be explanatory variables.

### a.

Draw side-by-side boxplots to compare the distribution of mood scores based on the music type.

Also side-by-side boxplots to compare the distribution of mood scores based on gender.

```{r}
library(ggplot2)
```

```{r}
music <- read.csv("mood.csv")
ggplot(music, aes(x = MusicType, y = MoodScore)) +
  geom_boxplot() +
  labs(x = "Music Type", y = "Mood Score", title = "Comparison of Mood Scores by Music Type")
```

```{r}
ggplot(music, aes(x = Gender, y = MoodScore)) +
  geom_boxplot() +
  labs(x = "Gender", y = "Mood Score", title = "Comparison of Mood Scores by Gender")
```

### b.

First, test whether the type of music has a significant effect on mood scores while ignoring gender in the model. (Hint: Use a partial F test. Write down the two regression model expressions used in the analysis.)

```{r}
model1 <- lm(MoodScore ~ MusicType, data = music)
model2 <- lm(MoodScore ~ MusicType + Gender, data = music)

# perform a partial F-test to compare the two models
summary(model2)
summary(model1)
anova(model2, model1)
```

**Ans:** The 'Adjusted-R' not significantly increase/decrease, means it is not significant enough while ignoring gender in the model. The partial-F test also shows this property with F = 0.0189

### c.

Then, test whether the type of music has a significant effect on mood scores while including gender in the model. (Hint: Use a partial F test. Write down the two regression model expressions used in the analysis.)

```{r}

# fit two linear regression models
model1 <- lm(MoodScore ~ MusicType, data = music)
model2 <- lm(MoodScore ~ MusicType + Gender, data = music)

# calculate the partial F-test
rss1 <- sum(residuals(model1)^2)
rss2 <- sum(residuals(model2)^2)
df1 <- df.residual(model1)
df2 <- df.residual(model2)
f_statistic <- ((rss1 - rss2) / (df2 - df1)) / (rss2 / df2)
f_statistic
```

**Comment: Not significant.**

### d.

Finally, test whether there exists an interaction effect between gender and the type of music on mood scores. (Hint: Use a t test. Write down the regression model expression used in the analysis.)

```{r}
# fit a linear regression model with an interaction term
model <- lm(MoodScore ~ Gender * MusicType, data = music)
# summary(model)

# extract the interaction coefficient and standard error
interaction_coef <- coef(model)["GenderMale:MusicTypeJazz"]
interaction_se <- sqrt(vcov(model)["GenderMale:MusicTypeJazz", "GenderMale:MusicTypeJazz"])

# calculate the t-statistic and degrees of freedom
t_stat <- interaction_coef / interaction_se
df <- df.residual(model)

# calculate the p-value
p_value <- 2 * pt(abs(t_stat), df = df, lower.tail = FALSE)
p_value

interaction_coef <- coef(model)["GenderMale:MusicTypePop"]
interaction_se <- sqrt(vcov(model)["GenderMale:MusicTypePop", "GenderMale:MusicTypePop"])

# calculate the t-statistic and degrees of freedom
t_stat <- interaction_coef / interaction_se
df <- df.residual(model)

# calculate the p-value
p_value <- 2 * pt(abs(t_stat), df = df, lower.tail = FALSE)
p_value
```

Comment: there's interaction between Gender and Music type. The male has interaction with Pop and Jazz music, but not very significant.
